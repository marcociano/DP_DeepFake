{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c049c75e-8622-45e3-8d22-08f159d067e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import timm\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d934d3-8519-4bf4-931a-c9fd78ffb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Controllo che torch sia installato con CUDA abilitato\n",
    "def check_cuda():\n",
    "    print(torch.version.cuda)\n",
    "    cuda_is_ok = torch.cuda.is_available()\n",
    "    print(f\"CUDA Enabled: {cuda_is_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4971006-88ee-414f-9926-145d1c7e94b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00527b4d-d57a-40e8-b4ef-212cd499a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b339558-6af2-4750-916c-bcdb35974c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory per i file di train\n",
    "DATA_DIR = ''\n",
    "\n",
    "#Grandezza del Batch (iperparametro)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Learning Rate (Iperparametro)\n",
    "LR = 0.001\n",
    "\n",
    "#Numero di epoche (Iperparametro)\n",
    "EPOCHS = 80\n",
    "\n",
    "DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1061cf99-c4d1-476a-bfa8-ba6284d51661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set: 15000\n",
      "Size of valid set: 4000\n",
      "Size of test set: 500\n"
     ]
    }
   ],
   "source": [
    "# Carica il file csv\n",
    "#df = pd.read_csv(\"spectrum_train_dataset.csv\")\n",
    "\n",
    "df_real = pd.read_csv(\"real_image_dataset.csv\")\n",
    "\n",
    "df_fake = pd.read_csv(\"fake_image_dataset.csv\")\n",
    "\n",
    "train_df_real, valid_df_real = train_test_split(df_real, test_size = 0.20, random_state = 42)\n",
    "train_df_fake, valid_df_fake = train_test_split(df_fake, test_size = 0.20, random_state = 42)\n",
    "\n",
    "test_df_real = train_df_real.sample(500)\n",
    "test_df_fake = train_df_fake.sample(500)\n",
    "train_df_real = train_df_real.drop(test_df_real.index)\n",
    "train_df_fake = train_df_fake.drop(test_df_fake.index)\n",
    "\n",
    "train_df = pd.concat([train_df_real, train_df_fake])\n",
    "valid_df = pd.concat([valid_df_real, valid_df_fake])\n",
    "test_df = pd.concat([test_df_real, test_df_fake])\n",
    "\n",
    "train_df= train_df.sample(frac= 1)\n",
    "valid_df= valid_df.sample(frac= 1)\n",
    "\n",
    "# Unisci tutti i dataframe in un unico dataframe finale per il train set\n",
    "final_df = pd.concat([train_df, valid_df])\n",
    "\n",
    "# Salva il dataframe finale di trian in un file CSV\n",
    "final_df.to_csv(\"spectrum_train_dataset.csv\", index=False)\n",
    "df = pd.read_csv(\"spectrum_train_dataset.csv\")\n",
    "\n",
    "# Creazione di liste concatenate delle immagini reali e false\n",
    "real_images = pd.concat([test_df_real['Anchor'], test_df_real['Positive']]).sample(500, random_state=42).tolist()\n",
    "fake_images = pd.concat([test_df_fake['Anchor'], test_df_fake['Positive']]).sample(500, random_state=42).tolist()\n",
    "\n",
    "# Assicurati che la lunghezza delle liste sia esattamente 500\n",
    "real_images = real_images[:500]\n",
    "fake_images = fake_images[:500]\n",
    "\n",
    "# Crea il dataframe del test set con due colonne: 'real' e 'fake'\n",
    "test_df = pd.DataFrame({\n",
    "    'real': real_images,\n",
    "    'fake': fake_images\n",
    "})\n",
    "\n",
    "# Salva il dataframe di test in un file CSV\n",
    "test_df.to_csv(\"test_set.csv\", index=False)\n",
    "\n",
    "print('Size of train set:', len(train_df))\n",
    "print('Size of valid set:', len(valid_df))\n",
    "print('Size of test set:', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d18e77-245e-46d9-bc4b-38042ede388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carico le immagini del file CSV che ho fornito, per settarle in modo corretto e darle in input alla rete neurale\n",
    "class APN_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.df.iloc[idx]\n",
    "\n",
    "    A_img = io.imread(DATA_DIR + row.Anchor, as_gray= True)\n",
    "    P_img = io.imread(DATA_DIR + row.Positive, as_gray= True)\n",
    "    N_img = io.imread(DATA_DIR + row.Negative, as_gray= True)\n",
    "\n",
    "    \n",
    "    #Permute because the third channel has to be in first channel in torch\n",
    "    A_img = np.expand_dims(A_img, 0)\n",
    "    P_img = np.expand_dims(P_img, 0)\n",
    "    N_img = np.expand_dims(N_img, 0)\n",
    "\n",
    "    A_img = torch.from_numpy(A_img)/ 255.0\n",
    "    P_img = torch.from_numpy(P_img)/ 255.0\n",
    "    N_img = torch.from_numpy(N_img)/ 255.0\n",
    "      \n",
    "    #A_img = torch.from_numpy(A_img.astype(np.int32)) / 65536.0\n",
    "    #P_img = torch.from_numpy(P_img.astype(np.int32)) / 65536.0\n",
    "    #N_img = torch.from_numpy(N_img.astype(np.int32)) / 65536.0\n",
    "\n",
    "    return A_img, P_img, N_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "155262e9-605e-47ac-b29b-206337ac7847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "data = APN_Dataset(train_df)\n",
    "item1, item2, item3 = data.__getitem__(0)\n",
    "print(item1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47edd68-032d-4893-a36c-127895563e56",
   "metadata": {},
   "source": [
    "Qui di seguito mi stampo il numero che compone l'insieme di dati di train, test e validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65299e8-9581-4595-a6f5-07e5568b0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainset: 15000\n",
      "Size of validset: 4000\n",
      "Size of validset: 500\n"
     ]
    }
   ],
   "source": [
    "trainset = APN_Dataset(train_df)\n",
    "validset = APN_Dataset(valid_df)\n",
    "testset = APN_Dataset(test_df)\n",
    "\n",
    "print(f\"Size of trainset: {len(trainset)}\")\n",
    "print(f\"Size of validset: {len(validset)}\")\n",
    "print(f\"Size of validset: {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0db331c-6d35-4fb3-8ceb-632e59b70a65",
   "metadata": {},
   "source": [
    "Carichiamo i dati di train e validation nella batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd299359-b3c3-4714-a276-3cd729ef3235",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "validloader = DataLoader(validset, batch_size = BATCH_SIZE)\n",
    "testloader = DataLoader(testset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240a522f-42d0-4499-ab22-7ddf7e8976e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of batches in trainloader : 469\n",
      "No. of batches in validloader : 125\n",
      "No. of batches in testloader : 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"No. of batches in trainloader : {len(trainloader)}\")\n",
    "print(f\"No. of batches in validloader : {len(validloader)}\")\n",
    "print(f\"No. of batches in testloader : {len(testloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158e1df-d8b4-42f9-9f21-0e5328847270",
   "metadata": {},
   "source": [
    "Questa funzione definisce un modello di rete neurale chiamato APN_Model, che carica un'architettura di rete preaddestrata e sostituisce il classificatore finale con un nuovo classificatore personalizzato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a6a526-fdac-4a44-9de8-d69a057a3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carico il modello di rete neurale\n",
    "class APN_Model(nn.Module):\n",
    "\n",
    "    #Viene definita la size del vettore di embedding\n",
    "  def __init__(self, emb_size = 512):\n",
    "    super(APN_Model, self).__init__()\n",
    "\n",
    "    #QUI CAIRCATE IL MODELLO, IN QUESTO CASO EFFICIENTNET VERSIONE B0 (LA PIÃ¹ LEGGERA DELLA FAMIGLIA)\n",
    "    self.efficientnet = timm.create_model('tf_efficientnetv2_b0', pretrained = False)\n",
    "    self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features = emb_size)\n",
    "\n",
    "  def forward(self, images):\n",
    "    embeddings = self.efficientnet(images)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2744ff4-084e-4ab3-ba35-1cfb5ece3706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUI FATE UNA PICCOLA MODIFICA ALLA RETE PER FARLE AVERE IN INPUT IMMAGINI IN SCALA DI GRIGIO DELLO SPETTRO DI FOURIER\n",
    "model = APN_Model()\n",
    "model.efficientnet.conv_stem = nn.Conv2d(1, 32, 3, 2, 1, bias=False);\n",
    "\n",
    "if model_weights == True :\n",
    "    model.load_state_dict(torch.load('trained_model_on_20000.pt'))\n",
    "\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a1eea0c-3caa-45d0-a4fd-bbfc9a71669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE DI TEST\n",
    "def test_fn(model, dataloader, criterion):\n",
    "  model.train() #ON Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():  \n",
    "    for A, P, N in tqdm(dataloader):\n",
    "        A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "        A_embs = model(A)\n",
    "        P_embs = model(P)\n",
    "        N_embs = model(N)\n",
    "      \n",
    "        loss = criterion(A_embs, P_embs, N_embs)\n",
    "      \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1e6e71-ad4e-44c3-a53e-10fcbd8fd7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE DI TRAINING\n",
    "def train_fn(model, dataloader, optimizer, criterion):\n",
    "  model.train() #ON Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  for A, P, N in tqdm(dataloader):\n",
    "    A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "    A_embs = model(A)\n",
    "    P_embs = model(P)\n",
    "    N_embs = model(N)\n",
    "\n",
    "    loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97248e6d-1321-48d8-a973-28f74e60eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNZIONE DI EVALUATION\n",
    "def eval_fn(model, dataloader, criterion):\n",
    "  model.eval() #OFF Dropout\n",
    "  total_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for A, P, N in tqdm(dataloader):\n",
    "      A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
    "\n",
    "      A_embs = model(A)\n",
    "      P_embs = model(P)\n",
    "      N_embs = model(N)\n",
    "\n",
    "      loss = criterion(A_embs, P_embs, N_embs)\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8730717b-a280-4a4e-9dab-a9398d9f1e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.TripletMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a2ac07f-cb59-48bd-b6c5-1b2fbb8d3ce5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training\n",
    "if model_weights == False :\n",
    "    best_valid_loss = np.Inf\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for i in range(EPOCHS):\n",
    "      train_loss = train_fn(model, trainloader, optimizer, criterion)\n",
    "      valid_loss = eval_fn(model, validloader, criterion)\n",
    "      training_loss.append(train_loss)\n",
    "      validation_loss.append(valid_loss)\n",
    "\n",
    "      if valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), 'trained_model_on_20000.pt')\n",
    "        best_valid_loss = valid_loss\n",
    "        print(\"SAVED_WEIGHTS_SUCCESS\")\n",
    "\n",
    "      print(f\"EPOCHS : {i+1} train_loss : {train_loss} valid_loss : {valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5f298eb-9e92-41c4-b43d-df7444372761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvataggio plottato dei dati\n",
    "if(model_weights == False):    \n",
    "    fig, axes = plt.subplots(2, 1, sharex= True, figsize=(10, 6))\n",
    "    axes[0].plot(training_loss)\n",
    "    axes[0].set_title('Training Loss')\n",
    "    axes[1].plot(validation_loss)\n",
    "    axes[1].set_title('Validation Loss')\n",
    "    plt.savefig('training_data5_20000(biggan)-80-epochs.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b23d2db-fa0b-4ba4-a67a-44d140829f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "#best_test_loss = np.Inf\n",
    "#test_loss = []\n",
    "\n",
    "#testing_loss = test_fn(model, testloader, criterion)\n",
    "\n",
    "#print(f\"test_loss : {testing_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34b5d02a-84f4-43ad-8aee-e69f83b185c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTA E' LA FUNZIONE PER GENERARE I VETTORI DI ENCODING\n",
    "def get_encoding_csv(model, anc_img_names, dirFolder):\n",
    "  anc_img_names_arr = np.array(anc_img_names)\n",
    "  encodings = []\n",
    "\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i in tqdm(anc_img_names_arr):\n",
    "      A = io.imread(dirFolder + i)\n",
    "      #A = torch.from_numpy(A).permute(2, 0, 1) / 255.0\n",
    "      A = np.expand_dims(A, 0)\n",
    "      A = torch.from_numpy(A.astype(np.int32)) / 255.0\n",
    "      A = A.to(DEVICE)\n",
    "      A_enc = model(A.unsqueeze(0))\n",
    "      encodings.append(A_enc.squeeze().cpu().detach().numpy())\n",
    "\n",
    "    encodings = np.array(encodings)\n",
    "    encodings = pd.DataFrame(encodings)\n",
    "    df_enc = pd.concat([anc_img_names, encodings], axis = 1)\n",
    "\n",
    "    return df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f80498d-2680-4a9c-8f79-409253582b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 19000/19000 [03:37<00:00, 87.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#QUI RICARICO IL MODELLO UNA VOLTA TRAINATO\n",
    "model.load_state_dict(torch.load('trained_model_on_20000.pt'))\n",
    "\n",
    "#QUI CREO IL DATABASE DI FEATURE VECTORS DEL TRAINING SET\n",
    "df_enc = get_encoding_csv(model, df['Anchor'], DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "942034ba-d74e-4bfc-8e4e-fa7e14eda8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anchor</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset\\big_gan\\big\\biggan-spectrum\\852\\img008...</td>\n",
       "      <td>-0.090214</td>\n",
       "      <td>-0.264717</td>\n",
       "      <td>-0.216647</td>\n",
       "      <td>0.032853</td>\n",
       "      <td>0.378233</td>\n",
       "      <td>0.116311</td>\n",
       "      <td>-0.175489</td>\n",
       "      <td>0.121259</td>\n",
       "      <td>0.067288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088355</td>\n",
       "      <td>0.235892</td>\n",
       "      <td>-0.088446</td>\n",
       "      <td>-0.125620</td>\n",
       "      <td>0.047661</td>\n",
       "      <td>0.344168</td>\n",
       "      <td>-0.177199</td>\n",
       "      <td>0.076804</td>\n",
       "      <td>-0.046667</td>\n",
       "      <td>0.137589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset\\coco\\coco\\coco2017\\train_spectrum\\img0...</td>\n",
       "      <td>-0.053666</td>\n",
       "      <td>-0.144451</td>\n",
       "      <td>0.146673</td>\n",
       "      <td>-0.082220</td>\n",
       "      <td>-0.090097</td>\n",
       "      <td>0.083666</td>\n",
       "      <td>-0.195972</td>\n",
       "      <td>-0.490259</td>\n",
       "      <td>-0.014724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020914</td>\n",
       "      <td>0.175289</td>\n",
       "      <td>-0.010938</td>\n",
       "      <td>-0.038295</td>\n",
       "      <td>-0.251522</td>\n",
       "      <td>-0.028093</td>\n",
       "      <td>0.342762</td>\n",
       "      <td>0.129735</td>\n",
       "      <td>0.114022</td>\n",
       "      <td>-0.140757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset\\big_gan\\big\\biggan-spectrum\\318\\img002...</td>\n",
       "      <td>-0.032845</td>\n",
       "      <td>-0.179673</td>\n",
       "      <td>-0.127086</td>\n",
       "      <td>-0.056337</td>\n",
       "      <td>0.268887</td>\n",
       "      <td>0.084888</td>\n",
       "      <td>-0.050379</td>\n",
       "      <td>-0.071240</td>\n",
       "      <td>-0.025579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115480</td>\n",
       "      <td>0.185573</td>\n",
       "      <td>-0.077517</td>\n",
       "      <td>-0.097880</td>\n",
       "      <td>-0.041625</td>\n",
       "      <td>0.187829</td>\n",
       "      <td>-0.021700</td>\n",
       "      <td>0.076438</td>\n",
       "      <td>-0.041155</td>\n",
       "      <td>0.152363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset\\big_gan\\big\\biggan-spectrum\\218\\img001...</td>\n",
       "      <td>-0.069160</td>\n",
       "      <td>-0.168132</td>\n",
       "      <td>-0.206332</td>\n",
       "      <td>-0.010379</td>\n",
       "      <td>0.451171</td>\n",
       "      <td>0.079230</td>\n",
       "      <td>-0.050864</td>\n",
       "      <td>0.157542</td>\n",
       "      <td>-0.005467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156118</td>\n",
       "      <td>0.196045</td>\n",
       "      <td>-0.110679</td>\n",
       "      <td>-0.162092</td>\n",
       "      <td>0.046835</td>\n",
       "      <td>0.317521</td>\n",
       "      <td>-0.188559</td>\n",
       "      <td>0.067422</td>\n",
       "      <td>-0.057077</td>\n",
       "      <td>0.224320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset\\big_gan\\big\\biggan-spectrum\\649\\img006...</td>\n",
       "      <td>-0.139863</td>\n",
       "      <td>-0.288083</td>\n",
       "      <td>-0.188613</td>\n",
       "      <td>0.095502</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>0.139068</td>\n",
       "      <td>-0.111750</td>\n",
       "      <td>0.102739</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106074</td>\n",
       "      <td>0.237030</td>\n",
       "      <td>-0.173063</td>\n",
       "      <td>-0.150995</td>\n",
       "      <td>0.037710</td>\n",
       "      <td>0.372867</td>\n",
       "      <td>-0.203903</td>\n",
       "      <td>0.093567</td>\n",
       "      <td>-0.026282</td>\n",
       "      <td>0.091340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Anchor         0         1  \\\n",
       "0  dataset\\big_gan\\big\\biggan-spectrum\\852\\img008... -0.090214 -0.264717   \n",
       "1  dataset\\coco\\coco\\coco2017\\train_spectrum\\img0... -0.053666 -0.144451   \n",
       "2  dataset\\big_gan\\big\\biggan-spectrum\\318\\img002... -0.032845 -0.179673   \n",
       "3  dataset\\big_gan\\big\\biggan-spectrum\\218\\img001... -0.069160 -0.168132   \n",
       "4  dataset\\big_gan\\big\\biggan-spectrum\\649\\img006... -0.139863 -0.288083   \n",
       "\n",
       "          2         3         4         5         6         7         8  ...  \\\n",
       "0 -0.216647  0.032853  0.378233  0.116311 -0.175489  0.121259  0.067288  ...   \n",
       "1  0.146673 -0.082220 -0.090097  0.083666 -0.195972 -0.490259 -0.014724  ...   \n",
       "2 -0.127086 -0.056337  0.268887  0.084888 -0.050379 -0.071240 -0.025579  ...   \n",
       "3 -0.206332 -0.010379  0.451171  0.079230 -0.050864  0.157542 -0.005467  ...   \n",
       "4 -0.188613  0.095502  0.383930  0.139068 -0.111750  0.102739  0.020576  ...   \n",
       "\n",
       "        502       503       504       505       506       507       508  \\\n",
       "0  0.088355  0.235892 -0.088446 -0.125620  0.047661  0.344168 -0.177199   \n",
       "1  0.020914  0.175289 -0.010938 -0.038295 -0.251522 -0.028093  0.342762   \n",
       "2  0.115480  0.185573 -0.077517 -0.097880 -0.041625  0.187829 -0.021700   \n",
       "3  0.156118  0.196045 -0.110679 -0.162092  0.046835  0.317521 -0.188559   \n",
       "4  0.106074  0.237030 -0.173063 -0.150995  0.037710  0.372867 -0.203903   \n",
       "\n",
       "        509       510       511  \n",
       "0  0.076804 -0.046667  0.137589  \n",
       "1  0.129735  0.114022 -0.140757  \n",
       "2  0.076438 -0.041155  0.152363  \n",
       "3  0.067422 -0.057077  0.224320  \n",
       "4  0.093567 -0.026282  0.091340  \n",
       "\n",
       "[5 rows x 513 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#QUI IL DATABASE COME CSV IN MODO TALE DA NON DOVER FARE QUESTA OPERAZIONE OGNI VOLTA\n",
    "#OVVIAMENTE, SE DEVO FARE UN NUOVO TRAINING DEVO ANCHE RICREARE GLI ENCODINGS\n",
    "df_enc.to_csv('database.csv', index = False)\n",
    "\n",
    "df_enc = pd.read_csv('database.csv')\n",
    "df_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "266ff1c7-5028-4e29-bd61-c6cd6be716e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(img_enc, anc_enc_arr):\n",
    "    #dist = np.sqrt(np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T))\n",
    "    dist = np.dot(img_enc-anc_enc_arr, (img_enc- anc_enc_arr).T)\n",
    "    #dist = np.sqrt(dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "530af24f-2baa-4e9b-8192-a9a5191ef6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageEmbeddings(img, model):\n",
    "\n",
    "    img = np.expand_dims(img, 0);\n",
    "    img = torch.from_numpy(img) / 255;\n",
    "    model.eval();\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img = img.to(DEVICE);\n",
    "        img_enc = model(img.unsqueeze(0));\n",
    "        img_enc = img_enc.detach().cpu().numpy();\n",
    "        img_enc = np.array(img_enc);\n",
    "\n",
    "    return img_enc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "947e62e6-0929-4948-9082-8702b5aa7868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchInDatabase(img_enc, database):\n",
    "    anc_enc_arr = database.iloc[:, 1:].to_numpy();\n",
    "    anc_img_names = database['Anchor'];\n",
    "\n",
    "    distance = [];\n",
    "    for i in range(anc_enc_arr.shape[0]):\n",
    "        dist = euclidean_dist(img_enc, anc_enc_arr[i : i+1, :]);\n",
    "        distance = np.append(distance, dist);\n",
    "\n",
    "    closest_idx = np.argsort(distance);\n",
    "\n",
    "    return database['Anchor'][closest_idx[0]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cdd831a-7745-448b-9d2f-180f88d081e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19000, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataTestReal = 'test_set.csv'\n",
    "y_true = []\n",
    "y_pred = []\n",
    "tempDf = df\n",
    "tempDf.head()\n",
    "tempDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d475fe4f-af74-49b2-8658-eaf9df6dac04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'fake'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fake'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Prendo i primi 500 Fake\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m tqdm(tempDf\u001b[38;5;241m.\u001b[39miterrows()):\n\u001b[1;32m----> 6\u001b[0m     img_name \u001b[38;5;241m=\u001b[39m DataTestReal \u001b[38;5;241m+\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcurrentTest\u001b[49m\u001b[43m]\u001b[49m;\n\u001b[0;32m      8\u001b[0m     img \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mimread(img_name);\n\u001b[0;32m     10\u001b[0m     img_enc \u001b[38;5;241m=\u001b[39m getImageEmbeddings(img, model);\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fake'"
     ]
    }
   ],
   "source": [
    "#Testo i fake\n",
    "currentTest = 'fake'\n",
    "database = df_enc\n",
    "# Prendo i primi 500 Fake\n",
    "for index, row in tqdm(tempDf.iterrows()):\n",
    "    img_name = DataTestReal + row[currentTest];\n",
    "\n",
    "    img = io.imread(img_name);\n",
    "\n",
    "    img_enc = getImageEmbeddings(img, model);\n",
    "\n",
    "    closestLabel = searchInDatabase(img_enc, database);\n",
    "\n",
    "    if \"real\" in closestLabel:\n",
    "        y_pred.append(\"real\");\n",
    "    else:\n",
    "        y_pred.append(\"fake\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bf7a9c-166b-492e-a361-adcd423ad494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006475ed-1dc6-4b4f-8ffb-094bd18c6940",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = df_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f0808d-3a4b-40b3-9da5-156e06c27dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testo i real\n",
    "currentTest = 'real'\n",
    "## Prendo i primi 500 Fake\n",
    "for index, row in tqdm(tempDf.iterrows()):\n",
    "    img_name = DataTestReal + row[currentTest]\n",
    "    img = io.imread(img_name)\n",
    "\n",
    "    img_enc = getImageEmbeddings(img, model)\n",
    "\n",
    "    closestLabel = searchInDatabase(img_enc, database)\n",
    "    if \"real\" in closestLabel:\n",
    "        y_pred.append(\"real\")\n",
    "    else:\n",
    "        y_pred.append(\"fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1d2ff-78c4-47a1-810b-bb37a25c2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_true))\n",
    "print(len(y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5211ed-dc2f-46a8-9e9b-5b1da8f4e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo i vettori di ground truth\n",
    "y_true = np.array(['fake'] * 1523)\n",
    "print(y_true.shape)\n",
    "\n",
    "temp = np.array(['real'] * 1523)\n",
    "print(temp.shape)\n",
    "\n",
    "y_true = np.concatenate([y_true, temp])\n",
    "print(y_true.shape)\n",
    "\n",
    "#Calcolo la matrice di confusione\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6919e-0506-4f06-9eae-9be47d1d063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estraggo dalla matrice di confusione i True Negative, False Positive, False Negative, True Positive\n",
    "TN, FP, FN, TP = confusion_matrix(y_true, y_pred, labels=[\"real\", \"fake\"]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746a9283-0526-418d-bb15-8f69fd29c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcolo alcune metriche per vedere come si comporta\n",
    "accuracy = round((TP + TN) /(TP + TN + FP + FN), 4) * 100\n",
    "precision = round((TP) / (TP + FP), 4) * 100\n",
    "sensitivy_recall = round((TP) / (TP + FN), 4) * 100\n",
    "specificity = round((TN) / (TN + FP) * 100, 4)\n",
    "F1_score = round((2* precision * sensitivy_recall) / (precision + sensitivy_recall), 2)\n",
    "\n",
    "print({\"Accuracy\":accuracy,\"Precision\":precision,\"Sensitivity_recall\":sensitivy_recall, \"Specificity\": specificity, \"F1_score\":F1_score})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
